{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e85d9e38",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['source', 'question', 'answer', 'rating'],\n",
              "        num_rows: 3831994\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['source', 'question', 'answer', 'rating'],\n",
              "        num_rows: 422786\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "ds = load_dataset(\"sapientinc/sudoku-extreme\")\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bba3262d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['source', 'question', 'answer', 'rating'],\n",
              "        num_rows: 1034600\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['source', 'question', 'answer', 'rating'],\n",
              "        num_rows: 114558\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Filter dataset to only include easy sources\n",
        "easy_sources = ['puzzles0_kaggle', 'puzzles1_unbiased', 'puzzles2_17_clue']\n",
        "easy_ds = ds.filter(lambda x: x['source'] in easy_sources)\n",
        "easy_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ad6fe549",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'puzzles1_unbiased',\n",
              " 'question': '..8..23412........9......5.........4..3..89.7....53....6.3.1.7...7.4.8...5.8.9...',\n",
              " 'answer': '678592341235184796914736258182967534543218967796453182869321475327645819451879623',\n",
              " 'rating': 0}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "easy_ds['train'][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3d8e2ac9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(81, 81)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(easy_ds['train'][0]['question']), len(easy_ds['train'][0]['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "11c54f21",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPLETE TRAINING PIPELINE FOR SUDOKU RNN\n",
        "# =============================================================================\n",
        "# This pipeline trains an RNN to solve Sudoku puzzles.\n",
        "# Key concepts:\n",
        "#   - Input: 81 cells, each one-hot encoded (10 dims: 0=unknown, 1-9=digits)\n",
        "#   - Output: 81 predictions, each classifying into 9 classes (digits 1-9)\n",
        "#   - Masking: We only compute loss on cells that were originally unknown ('.')\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import (\n",
        "    DataLoader,\n",
        "    Dataset,\n",
        ")\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e9cd5afa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample shapes:\n",
            "  question: torch.Size([81, 10])\n",
            "  answer: torch.Size([81])\n",
            "  mask: torch.Size([81])\n",
            "  mask sum (unknown cells): 56\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 1: CUSTOM DATASET CLASS\n",
        "# =============================================================================\n",
        "# PyTorch Dataset that:\n",
        "#   1. Converts question string to one-hot tensor (81, 10)\n",
        "#   2. Converts answer to class indices (81,) - shifted to 0-8 for CrossEntropyLoss\n",
        "#   3. Creates mask: True where cell was unknown (we need to predict these)\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "class SudokuDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for Sudoku puzzles.\n",
        "\n",
        "    Each sample contains:\n",
        "        - question: One-hot encoded input puzzle (81, 10)\n",
        "        - answer: Target class indices for each cell (81,) with values 0-8\n",
        "        - mask: Boolean mask where True = cell needs prediction (81,)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hf_dataset):\n",
        "        \"\"\"Initialize dataset from HuggingFace dataset.\n",
        "\n",
        "        Args:\n",
        "            hf_dataset: HuggingFace dataset with 'question' and 'answer' columns.\n",
        "        \"\"\"\n",
        "        self.data = hf_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get a single sample by index.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of the sample.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing question (one-hot), answer (class indices), and mask.\n",
        "        \"\"\"\n",
        "        row = self.data[idx]\n",
        "        question_str = row['question']\n",
        "        answer_str = row['answer']\n",
        "\n",
        "        # Step 1: Parse question string\n",
        "        # '.' means unknown cell -> we use 0 to represent it\n",
        "        # Known cells have digits 1-9\n",
        "        question_ints = [0 if c == '.' else int(c) for c in question_str]\n",
        "\n",
        "        # Step 2: Create mask BEFORE one-hot encoding\n",
        "        # mask[i] = True means cell i was unknown and needs prediction\n",
        "        mask = torch.tensor([q == 0 for q in question_ints], dtype=torch.bool)\n",
        "\n",
        "        # Step 3: One-hot encode the question\n",
        "        # Shape: (81, 10) - each cell has 10 possible values (0-9)\n",
        "        question_tensor = torch.tensor(question_ints, dtype=torch.long)\n",
        "        question_onehot = F.one_hot(question_tensor, num_classes=10).float()\n",
        "\n",
        "        # Step 4: Parse answer and shift to 0-8 range\n",
        "        # CrossEntropyLoss expects class indices starting from 0\n",
        "        # Original digits are 1-9, so we subtract 1 to get 0-8\n",
        "        answer_ints = [int(c) - 1 for c in answer_str]\n",
        "        answer_tensor = torch.tensor(answer_ints, dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'question': question_onehot,  # (81, 10) - input to RNN\n",
        "            'answer': answer_tensor,       # (81,) - target class indices 0-8\n",
        "            'mask': mask,                  # (81,) - True where we need prediction\n",
        "        }\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SudokuDataset(easy_ds['train'])\n",
        "test_dataset = SudokuDataset(easy_ds['test'])\n",
        "\n",
        "# Verify shapes\n",
        "sample = train_dataset[0]\n",
        "print('Sample shapes:')\n",
        "print(f\"  question: {sample['question'].shape}\")  # Should be (81, 10)\n",
        "print(f\"  answer: {sample['answer'].shape}\")       # Should be (81,)\n",
        "print(f\"  mask: {sample['mask'].shape}\")           # Should be (81,)\n",
        "print(f\"  mask sum (unknown cells): {sample['mask'].sum().item()}\")  # How many cells to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ed656e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Total parameters: 33,653,769\n",
            "SudokuLSTM(\n",
            "  (lstm): LSTM(10, 512, num_layers=6, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (fc): Linear(in_features=1024, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 2: IMPROVED RNN MODEL\n",
        "# =============================================================================\n",
        "# Architecture:\n",
        "#   Input: (batch, 81, 10) - 81 cells, each one-hot encoded\n",
        "#   RNN: Processes sequence, outputs (batch, 81, hidden_size)\n",
        "#   FC: Maps hidden to 9 classes, outputs (batch, 81, 9)\n",
        "#\n",
        "# Using LSTM instead of vanilla RNN for better gradient flow.\n",
        "# Using bidirectional to capture constraints from both directions.\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "class SudokuLSTM(nn.Module):\n",
        "    \"\"\"LSTM model for solving Sudoku puzzles.\n",
        "\n",
        "    Processes the 81 cells as a sequence and predicts the digit (1-9)\n",
        "    for each cell. Uses bidirectional LSTM to capture constraints\n",
        "    from both directions in the sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=10,\n",
        "        hidden_size=128,\n",
        "        num_layers=2,\n",
        "        num_classes=9,\n",
        "        dropout=0.2,\n",
        "        bidirectional=True,\n",
        "    ):\n",
        "        \"\"\"Initialize the LSTM model.\n",
        "\n",
        "        Args:\n",
        "            input_size: Dimension of input features (10 for one-hot).\n",
        "            hidden_size: Number of LSTM hidden units.\n",
        "            num_layers: Number of stacked LSTM layers.\n",
        "            num_classes: Number of output classes (9 for digits 1-9).\n",
        "            dropout: Dropout rate between LSTM layers.\n",
        "            bidirectional: Whether to use bidirectional LSTM.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # LSTM layer\n",
        "        # Input: (batch, seq_len=81, input_size=10)\n",
        "        # Output: (batch, seq_len=81, hidden_size * num_directions)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional,\n",
        "        )\n",
        "\n",
        "        # Fully connected layer to map LSTM output to class predictions\n",
        "        # If bidirectional, LSTM output has size hidden_size * 2\n",
        "        fc_input_size = hidden_size * 2 if bidirectional else hidden_size\n",
        "        self.fc = nn.Linear(fc_input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, 81, 10).\n",
        "\n",
        "        Returns:\n",
        "            Logits tensor of shape (batch, 81, 9).\n",
        "        \"\"\"\n",
        "        # x shape: (batch, 81, 10)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        # lstm_out shape: (batch, 81, hidden_size * num_directions)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Pass through fully connected layer\n",
        "        # out shape: (batch, 81, 9)\n",
        "        out = self.fc(lstm_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Create model\n",
        "# Device selection: prefer CUDA > MPS (Apple Silicon) > CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "model = SudokuLSTM(\n",
        "    input_size=10,\n",
        "    hidden_size=512,   # Increased from 128\n",
        "    num_layers=6,      # Increased from 2\n",
        "    num_classes=9,\n",
        "    dropout=0.3,       # Slightly more regularization\n",
        "    bidirectional=True,\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'Total parameters: {total_params:,}')\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "4eab04c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 2.5333\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 3: MASKED LOSS FUNCTION\n",
        "# =============================================================================\n",
        "# Key insight: We only want to compute loss on cells that were UNKNOWN.\n",
        "# Known cells should not contribute to the loss because the model\n",
        "# doesn't need to \"learn\" to keep them unchanged.\n",
        "#\n",
        "# How it works:\n",
        "#   1. Compute CrossEntropyLoss for ALL cells (reduction='none')\n",
        "#   2. Multiply by mask (True=1 for unknown, False=0 for known)\n",
        "#   3. Average only over the masked positions\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def masked_cross_entropy_loss(predictions, targets, mask):\n",
        "    \"\"\"Compute CrossEntropyLoss only on masked (unknown) cells.\n",
        "\n",
        "    Args:\n",
        "        predictions: Model output logits, shape (batch, 81, 9).\n",
        "        targets: Target class indices, shape (batch, 81).\n",
        "        mask: Boolean mask, shape (batch, 81). True = compute loss.\n",
        "\n",
        "    Returns:\n",
        "        Scalar loss value (averaged over all masked positions).\n",
        "    \"\"\"\n",
        "    batch_size = predictions.shape[0]\n",
        "\n",
        "    # Reshape for CrossEntropyLoss\n",
        "    # CrossEntropyLoss expects: (N, C) predictions and (N,) targets\n",
        "    # where C = number of classes\n",
        "    predictions_flat = predictions.view(-1, 9)  # (batch * 81, 9)\n",
        "    targets_flat = targets.view(-1)              # (batch * 81,)\n",
        "\n",
        "    # Compute loss for each position (no reduction)\n",
        "    loss_per_position = F.cross_entropy(\n",
        "        predictions_flat,\n",
        "        targets_flat,\n",
        "        reduction='none',  # Don't reduce - we'll mask first\n",
        "    )\n",
        "\n",
        "    # Reshape back to (batch, 81)\n",
        "    loss_per_position = loss_per_position.view(batch_size, 81)\n",
        "\n",
        "    # Apply mask: only count loss where mask is True\n",
        "    # mask.float() converts True->1.0, False->0.0\n",
        "    masked_loss = loss_per_position * mask.float()\n",
        "\n",
        "    # Average over masked positions only\n",
        "    # Sum all losses, divide by number of True values in mask\n",
        "    num_masked = mask.sum()\n",
        "\n",
        "    if num_masked > 0:\n",
        "        loss = masked_loss.sum() / num_masked\n",
        "    else:\n",
        "        # Edge case: no masked positions (shouldn't happen in practice)\n",
        "        loss = masked_loss.sum()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Test the loss function with dummy data\n",
        "dummy_pred = torch.randn(2, 81, 9)  # 2 samples, 81 cells, 9 classes\n",
        "dummy_target = torch.randint(0, 9, (2, 81))  # Random targets 0-8\n",
        "dummy_mask = torch.rand(2, 81) > 0.5  # Random mask\n",
        "\n",
        "loss = masked_cross_entropy_loss(dummy_pred, dummy_target, dummy_mask)\n",
        "print(f'Test loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c6cfec75",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 4: ACCURACY METRICS\n",
        "# =============================================================================\n",
        "# We track two types of accuracy:\n",
        "#   1. Cell accuracy: % of individual cells predicted correctly (masked only)\n",
        "#   2. Puzzle accuracy: % of puzzles completely solved correctly\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions, targets, mask):\n",
        "    \"\"\"Compute cell-level and puzzle-level accuracy.\n",
        "\n",
        "    Args:\n",
        "        predictions: Model output logits, shape (batch, 81, 9).\n",
        "        targets: Target class indices, shape (batch, 81).\n",
        "        mask: Boolean mask, shape (batch, 81). True = cell needs prediction.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cell_accuracy, puzzle_accuracy).\n",
        "    \"\"\"\n",
        "    # Get predicted classes (argmax over the 9 classes)\n",
        "    predicted_classes = predictions.argmax(dim=-1)  # (batch, 81)\n",
        "\n",
        "    # Cell accuracy: correct predictions among masked cells\n",
        "    correct = (predicted_classes == targets) & mask  # Both correct AND masked\n",
        "    cell_accuracy = correct.sum().float() / mask.sum().float()\n",
        "\n",
        "    # Puzzle accuracy: all masked cells correct for each puzzle\n",
        "    # For each puzzle, check if ALL masked positions are correct\n",
        "    # A puzzle is solved if: for all positions where mask=True, prediction=target\n",
        "    correct_per_puzzle = correct.sum(dim=1)  # (batch,) - correct cells per puzzle\n",
        "    masked_per_puzzle = mask.sum(dim=1)       # (batch,) - masked cells per puzzle\n",
        "    puzzles_solved = (correct_per_puzzle == masked_per_puzzle).float()\n",
        "    puzzle_accuracy = puzzles_solved.mean()\n",
        "\n",
        "    return cell_accuracy.item(), puzzle_accuracy.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "516acf60",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 5: TRAINING AND EVALUATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, device):\n",
        "    \"\"\"Train the model for one epoch.\n",
        "\n",
        "    Args:\n",
        "        model: The neural network model.\n",
        "        dataloader: DataLoader for training data.\n",
        "        optimizer: Optimizer for updating weights.\n",
        "        device: Device to run on (cuda/cpu).\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with average loss and accuracies for the epoch.\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_cell_acc = 0.0\n",
        "    total_puzzle_acc = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    # Progress bar\n",
        "    pbar = tqdm(dataloader, desc='Training')\n",
        "\n",
        "    for batch in pbar:\n",
        "        # Move data to device\n",
        "        questions = batch['question'].to(device)  # (batch, 81, 10)\n",
        "        answers = batch['answer'].to(device)       # (batch, 81)\n",
        "        masks = batch['mask'].to(device)           # (batch, 81)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(questions)  # (batch, 81, 9)\n",
        "\n",
        "        # Compute masked loss\n",
        "        loss = masked_cross_entropy_loss(predictions, answers, masks)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute accuracy\n",
        "        with torch.no_grad():\n",
        "            cell_acc, puzzle_acc = compute_accuracy(predictions, answers, masks)\n",
        "\n",
        "        # Accumulate metrics\n",
        "        total_loss += loss.item()\n",
        "        total_cell_acc += cell_acc\n",
        "        total_puzzle_acc += puzzle_acc\n",
        "        num_batches += 1\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'cell_acc': f'{cell_acc:.2%}',\n",
        "            'puzzle_acc': f'{puzzle_acc:.2%}',\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        'loss': total_loss / num_batches,\n",
        "        'cell_accuracy': total_cell_acc / num_batches,\n",
        "        'puzzle_accuracy': total_puzzle_acc / num_batches,\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    \"\"\"Evaluate the model on a dataset.\n",
        "\n",
        "    Args:\n",
        "        model: The neural network model.\n",
        "        dataloader: DataLoader for evaluation data.\n",
        "        device: Device to run on (cuda/cpu).\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with average loss and accuracies.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_cell_acc = 0.0\n",
        "    total_puzzle_acc = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradient computation during evaluation\n",
        "        pbar = tqdm(dataloader, desc='Evaluating')\n",
        "\n",
        "        for batch in pbar:\n",
        "            questions = batch['question'].to(device)\n",
        "            answers = batch['answer'].to(device)\n",
        "            masks = batch['mask'].to(device)\n",
        "\n",
        "            predictions = model(questions)\n",
        "            loss = masked_cross_entropy_loss(predictions, answers, masks)\n",
        "            cell_acc, puzzle_acc = compute_accuracy(predictions, answers, masks)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_cell_acc += cell_acc\n",
        "            total_puzzle_acc += puzzle_acc\n",
        "            num_batches += 1\n",
        "\n",
        "    return {\n",
        "        'loss': total_loss / num_batches,\n",
        "        'cell_accuracy': total_cell_acc / num_batches,\n",
        "        'puzzle_accuracy': total_puzzle_acc / num_batches,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "cb3c04b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 1,034,600\n",
            "Test samples: 114,558\n",
            "Batches per epoch: 8,083\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 6: MAIN TRAINING LOOP\n",
        "# =============================================================================\n",
        "# Hyperparameters and training configuration\n",
        "# =============================================================================\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 2\n",
        "# NOTE: num_workers=0 is required in Jupyter notebooks on macOS\n",
        "# because multiprocessing can't pickle classes defined in the notebook.\n",
        "# pin_memory=False because MPS (Apple Silicon) doesn't support it.\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "# Create DataLoaders\n",
        "# shuffle=True for training to randomize order each epoch\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,  # No need to shuffle test data\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=False,\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "# Adam is a good default choice for most deep learning tasks\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler (optional)\n",
        "# Reduces LR when validation loss plateaus\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "\n",
        ")\n",
        "\n",
        "print(f'Training samples: {len(train_dataset):,}')\n",
        "print(f'Test samples: {len(test_dataset):,}')\n",
        "print(f'Batches per epoch: {len(train_loader):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "700424c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Epoch 1/2\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  31%|███       | 2494/8083 [30:34<1:07:58,  1.37it/s, loss=1.5235, cell_acc=31.72%, puzzle_acc=0.00%]"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# STEP 7: RUN TRAINING\n",
        "# =============================================================================\n",
        "# This is where the actual training happens.\n",
        "# For each epoch:\n",
        "#   1. Train on all training batches\n",
        "#   2. Evaluate on test set\n",
        "#   3. Update learning rate if needed\n",
        "#   4. Save best model\n",
        "# =============================================================================\n",
        "\n",
        "best_puzzle_acc = 0.0\n",
        "history = {'train': [], 'test': []}\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
        "    print(f'{\"=\"*60}')\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_metrics = train_one_epoch(model, train_loader, optimizer, device)\n",
        "    print(f'\\nTrain - Loss: {train_metrics[\"loss\"]:.4f}, '\n",
        "          f'Cell Acc: {train_metrics[\"cell_accuracy\"]:.2%}, '\n",
        "          f'Puzzle Acc: {train_metrics[\"puzzle_accuracy\"]:.2%}')\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_metrics = evaluate(model, test_loader, device)\n",
        "    print(f'Test  - Loss: {test_metrics[\"loss\"]:.4f}, '\n",
        "          f'Cell Acc: {test_metrics[\"cell_accuracy\"]:.2%}, '\n",
        "          f'Puzzle Acc: {test_metrics[\"puzzle_accuracy\"]:.2%}')\n",
        "\n",
        "    # Update learning rate based on test loss\n",
        "    scheduler.step(test_metrics['loss'])\n",
        "\n",
        "    # Save history\n",
        "    history['train'].append(train_metrics)\n",
        "    history['test'].append(test_metrics)\n",
        "\n",
        "    # Save best model\n",
        "    if test_metrics['puzzle_accuracy'] > best_puzzle_acc:\n",
        "        best_puzzle_acc = test_metrics['puzzle_accuracy']\n",
        "        torch.save(model.state_dict(), 'best_sudoku_model.pt')\n",
        "        print(f'  → New best model saved! Puzzle accuracy: {best_puzzle_acc:.2%}')\n",
        "\n",
        "print(f'\\n{\"=\"*60}')\n",
        "print(f'Training complete! Best puzzle accuracy: {best_puzzle_acc:.2%}')\n",
        "print(f'{\"=\"*60}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b506ac82",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 8: INFERENCE - TEST ON A SINGLE PUZZLE\n",
        "# =============================================================================\n",
        "# After training, let's see how the model performs on a sample puzzle.\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def solve_sudoku(model, puzzle_string, device):\n",
        "    \"\"\"Solve a single Sudoku puzzle using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained SudokuLSTM model.\n",
        "        puzzle_string: 81-character string with '.' for unknown cells.\n",
        "        device: Device to run inference on.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (predicted_string, confidence_scores).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Convert puzzle string to one-hot tensor\n",
        "    question_ints = [0 if c == '.' else int(c) for c in puzzle_string]\n",
        "    question_tensor = torch.tensor(question_ints, dtype=torch.long)\n",
        "    question_onehot = F.one_hot(question_tensor, num_classes=10).float()\n",
        "\n",
        "    # Add batch dimension and move to device\n",
        "    input_tensor = question_onehot.unsqueeze(0).to(device)  # (1, 81, 10)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)  # (1, 81, 9)\n",
        "        probabilities = F.softmax(output, dim=-1)  # Convert to probabilities\n",
        "        predictions = output.argmax(dim=-1)  # (1, 81) - class indices 0-8\n",
        "\n",
        "    # Convert predictions back to digits 1-9\n",
        "    predictions = predictions.squeeze(0).cpu()  # (81,)\n",
        "    probabilities = probabilities.squeeze(0).cpu()  # (81, 9)\n",
        "\n",
        "    # Build result string\n",
        "    result = []\n",
        "    confidences = []\n",
        "    for i, c in enumerate(puzzle_string):\n",
        "        if c == '.':\n",
        "            # Unknown cell - use model prediction\n",
        "            pred_digit = predictions[i].item() + 1  # Convert 0-8 back to 1-9\n",
        "            confidence = probabilities[i].max().item()\n",
        "            result.append(str(pred_digit))\n",
        "            confidences.append(confidence)\n",
        "        else:\n",
        "            # Known cell - keep original\n",
        "            result.append(c)\n",
        "            confidences.append(1.0)\n",
        "\n",
        "    return ''.join(result), confidences\n",
        "\n",
        "\n",
        "def display_sudoku(puzzle_str, title='Sudoku'):\n",
        "    \"\"\"Display a Sudoku puzzle in a readable format.\n",
        "\n",
        "    Args:\n",
        "        puzzle_str: 81-character string representing the puzzle.\n",
        "        title: Title to display above the puzzle.\n",
        "    \"\"\"\n",
        "    print(f'\\n{title}:')\n",
        "    print('+-------+-------+-------+')\n",
        "    for i in range(9):\n",
        "        row = puzzle_str[i*9:(i+1)*9]\n",
        "        formatted = ' | '.join(\n",
        "            ' '.join(row[j*3:(j+1)*3]) for j in range(3)\n",
        "        )\n",
        "        print(f'| {formatted} |')\n",
        "        if i % 3 == 2:\n",
        "            print('+-------+-------+-------+')\n",
        "\n",
        "\n",
        "# Test on a sample from the test set\n",
        "sample_idx = 0\n",
        "sample = easy_ds['test'][sample_idx]\n",
        "puzzle = sample['question']\n",
        "solution = sample['answer']\n",
        "\n",
        "print('Testing trained model on a sample puzzle:')\n",
        "display_sudoku(puzzle, 'Input Puzzle')\n",
        "display_sudoku(solution, 'Ground Truth')\n",
        "\n",
        "# Solve using model\n",
        "predicted, confidences = solve_sudoku(model, puzzle, device)\n",
        "display_sudoku(predicted, 'Model Prediction')\n",
        "\n",
        "# Check accuracy\n",
        "correct = sum(p == s for p, s in zip(predicted, solution))\n",
        "print(f'\\nAccuracy: {correct}/81 cells correct ({correct/81:.1%})')\n",
        "print(f'Average confidence: {sum(confidences)/len(confidences):.2%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59cd4444",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 9: VISUALIZE TRAINING HISTORY\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Plot loss\n",
        "epochs = range(1, len(history['train']) + 1)\n",
        "axes[0].plot(epochs, [h['loss'] for h in history['train']], 'b-', label='Train')\n",
        "axes[0].plot(epochs, [h['loss'] for h in history['test']], 'r-', label='Test')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Loss over Epochs')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Plot cell accuracy\n",
        "axes[1].plot(epochs, [h['cell_accuracy'] for h in history['train']], 'b-', label='Train')\n",
        "axes[1].plot(epochs, [h['cell_accuracy'] for h in history['test']], 'r-', label='Test')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Cell Accuracy')\n",
        "axes[1].set_title('Cell Accuracy over Epochs')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Plot puzzle accuracy\n",
        "axes[2].plot(epochs, [h['puzzle_accuracy'] for h in history['train']], 'b-', label='Train')\n",
        "axes[2].plot(epochs, [h['puzzle_accuracy'] for h in history['test']], 'r-', label='Test')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('Puzzle Accuracy')\n",
        "axes[2].set_title('Puzzle Accuracy over Epochs')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93b71c32",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c7213d",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
