{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Improved Sudoku Solver Training\n",
        "\n",
        "This notebook implements an optimized pipeline for training a Sudoku solver. \n",
        "Key improvements over the original:\n",
        "1.  **Offline Binary Data**: Pre-processes data to simple integer arrays (`.npy`), avoiding slow string parsing during training.\n",
        "2.  **Embeddings**: Uses `nn.Embedding` instead of One-Hot encoding for better memory efficiency.\n",
        "3.  **Data Augmentation**: Implements Sudoku-valid permutations (rows, cols, digits) to expand the dataset.\n",
        "4.  **Mixed Precision**: Uses `torch.amp` for faster training.\n",
        "5.  **Large Batch Size**: Enabled by the above optimizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Processing\n",
        "We defines functions to download, augment, and save the dataset as binary files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from HuggingFace...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "380ccda577f2498ea81d6108bc058362",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "941d81305a16427396725bd049f329fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train.csv:   0%|          | 0.00/719M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a668d0a3d94c41a3a8766bd88d42915b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test.csv:   0%|          | 0.00/79.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69f50538672d4da1acd0a6f5bdf0752c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/3831994 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31b11b318a6847998290bf66ba1ec1d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/422786 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing train split...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95956e4035064102815f208c4611e691",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/3831994 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting to integers and augmenting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1034600/1034600 [04:16<00:00, 4026.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving 2069200 samples to data/processed...\n",
            "Processing test split...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f33d69645a74019a35d78ee13a6505b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/422786 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting to integers and augmenting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 114558/114558 [00:13<00:00, 8811.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving 114558 samples to data/processed...\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def shuffle_sudoku(board_flat, solution_flat):\n",
        "    \"\"\"Apply valid Sudoku transformations (permutations) to a board and solution.\"\"\"\n",
        "    # Reshape to 9x9\n",
        "    board = board_flat.reshape(9, 9)\n",
        "    sol = solution_flat.reshape(9, 9)\n",
        "    \n",
        "    # 1. Permute digits (1-9)\n",
        "    # Create a mapping: 0->0 (unknown), 1-9 -> permuted 1-9\n",
        "    digit_map = np.arange(10)\n",
        "    digit_map[1:] = np.random.permutation(np.arange(1, 10))\n",
        "    \n",
        "    # 2. Random Transpose\n",
        "    if np.random.rand() < 0.5:\n",
        "        board = board.T\n",
        "        sol = sol.T\n",
        "        \n",
        "    # 3. Permute Bands (groups of 3 rows)\n",
        "    bands = np.random.permutation(3)\n",
        "    row_perm = np.concatenate([b * 3 + np.random.permutation(3) for b in bands])\n",
        "    \n",
        "    # 4. Permute Stacks (groups of 3 cols)\n",
        "    stacks = np.random.permutation(3)\n",
        "    col_perm = np.concatenate([s * 3 + np.random.permutation(3) for s in stacks])\n",
        "    \n",
        "    # Apply permutations\n",
        "    # We can do this by indexing: new_board[i, j] = old_board[row_perm[i], col_perm[j]]\n",
        "    # Or simpler: reorder rows, then reorder cols\n",
        "    board = board[row_perm, :][:, col_perm]\n",
        "    sol = sol[row_perm, :][:, col_perm]\n",
        "    \n",
        "    # Map digits\n",
        "    board = digit_map[board]\n",
        "    sol = digit_map[sol]\n",
        "    \n",
        "    return board.flatten(), sol.flatten()\n",
        "\n",
        "def preprocess_dataset(output_dir=\"data/processed\", num_aug=1):\n",
        "    \"\"\"Download, filter, augment, and save dataset as .npy.\"\"\"\n",
        "    if os.path.exists(output_dir):\n",
        "        print(f\"Dataset already exists at {output_dir}. Skipping generation.\")\n",
        "        return\n",
        "        \n",
        "    print(\"Loading dataset from HuggingFace...\")\n",
        "    ds = load_dataset(\"sapientinc/sudoku-extreme\")\n",
        "    \n",
        "    # Filter easy sources\n",
        "    easy_sources = ['puzzles0_kaggle', 'puzzles1_unbiased', 'puzzles2_17_clue']\n",
        "    \n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    for split in ['train', 'test']:\n",
        "        print(f\"Processing {split} split...\")\n",
        "        # Filter\n",
        "        split_ds = ds[split].filter(lambda x: x['source'] in easy_sources)\n",
        "        \n",
        "        questions = []\n",
        "        answers = []\n",
        "        \n",
        "        print(\"Converting to integers and augmenting...\")\n",
        "        for item in tqdm(split_ds):\n",
        "            # Parse strings one last time\n",
        "            # '.' -> 0, '1'-'9' -> 1-9\n",
        "            q = np.array([0 if c == '.' else int(c) for c in item['question']], dtype=np.uint8)\n",
        "            # Answer is 1-9. We keep it 1-9 for now (0 means N/A if needed, but answers are full)\n",
        "            # Original code shifted answer to 0-8. Here we keep 1-9 to match input features,\n",
        "            # but will shift for loss calculation if needed.\n",
        "            a = np.array([int(c) for c in item['answer']], dtype=np.uint8)\n",
        "            \n",
        "            # Original data\n",
        "            questions.append(q)\n",
        "            answers.append(a)\n",
        "            \n",
        "            # Augmentations (only for train)\n",
        "            if split == 'train' and num_aug > 0:\n",
        "                for _ in range(num_aug):\n",
        "                    q_aug, a_aug = shuffle_sudoku(q, a)\n",
        "                    questions.append(q_aug)\n",
        "                    answers.append(a_aug)\n",
        "        \n",
        "        # Save as .npy\n",
        "        q_arr = np.array(questions, dtype=np.uint8)\n",
        "        a_arr = np.array(answers, dtype=np.uint8)\n",
        "        \n",
        "        print(f\"Saving {len(q_arr)} samples to {output_dir}...\")\n",
        "        np.save(os.path.join(output_dir, f\"{split}_questions.npy\"), q_arr)\n",
        "        np.save(os.path.join(output_dir, f\"{split}_answers.npy\"), a_arr)\n",
        "\n",
        "# Run preprocessing (will skip if already exists)\n",
        "preprocess_dataset(num_aug=1)  # 1 augmentation -> 2x dataset size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Optimized Dataset Class\n",
        "Loads data directly from memory-mapped `.npy` files. Instant access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FastSudokuDataset(Dataset):\n",
        "    def __init__(self, data_dir, split):\n",
        "        self.questions = np.load(os.path.join(data_dir, f\"{split}_questions.npy\"))\n",
        "        self.answers = np.load(os.path.join(data_dir, f\"{split}_answers.npy\"))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Data is already uint8 0-9\n",
        "        # q: (81,)\n",
        "        # a: (81,)\n",
        "        q = self.questions[idx].astype(np.int64) # Long for embedding\n",
        "        a = self.answers[idx].astype(np.int64)\n",
        "        \n",
        "        # Create mask (where q was 0)\n",
        "        # Note: In embedding, we can just feed 0. \n",
        "        # But for loss we need mask.\n",
        "        mask = (q == 0)\n",
        "        \n",
        "        # Target for Loss: PyTorch CrossEntropy expects 0-8 for classes 0-8.\n",
        "        # Our answers are 1-9. So we subtract 1.\n",
        "        target = a - 1\n",
        "        \n",
        "        return {\n",
        "            'question': torch.from_numpy(q),    # (81,) Indices 0-9\n",
        "            'answer': torch.from_numpy(target), # (81,) Indices 0-8\n",
        "            'mask': torch.from_numpy(mask)      # (81,) Bool\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Improved Model with Embeddings\n",
        "Replaced One-Hot input with `nn.Embedding`. \n",
        "- Input dimension dropped from `(B, 81, 10)` to `(B, 81)` indices.\n",
        "- Memory usage significantly reduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SudokuLSTM_Improved(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size=512,\n",
        "        num_layers=6,\n",
        "        dropout=0.3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Embedding layer\n",
        "        # 10 possible values in input: 0 (unknown) + 1-9 (digits)\n",
        "        self.embedding = nn.Embedding(10, hidden_size)\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "\n",
        "        # Output maps to 9 classes (digits 1-9)\n",
        "        self.fc = nn.Linear(hidden_size * 2, 9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, 81) indices\n",
        "        # embed: (batch, 81, hidden)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # lstm_out: (batch, 81, hidden * 2)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        \n",
        "        # out: (batch, 81, 9)\n",
        "        out = self.fc(lstm_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Optimized Training Loop\n",
        "- **AMP (Automatic Mixed Precision)**: `torch.amp.autocast`\n",
        "- **Larger Batch Size**\n",
        "- **Gradient Clipping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Compiling model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1889909573.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 76/2021 [01:30<38:42,  1.19s/it, loss=2.0504] \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1889909573.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average Loss: {loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1889909573.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, scaler)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m         )\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    355\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    355\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Config\n",
        "BATCH_SIZE = 1024  # Increased from 128\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 20    # Can run many more due to speed\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load Data\n",
        "train_ds = FastSudokuDataset(\"data/processed\", \"train\")\n",
        "test_ds = FastSudokuDataset(\"data/processed\", \"test\")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "# Model Setup\n",
        "model = SudokuLSTM_Improved().to(device)\n",
        "\n",
        "# Compile model if available (Linux/CUDA usually)\n",
        "if hasattr(torch, 'compile') and device.type == 'cuda':\n",
        "    print(\"Compiling model...\")\n",
        "    model = torch.compile(model)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
        "\n",
        "def masked_loss(preds, targets, mask):\n",
        "    \"\"\"Compute CrossEntropyLoss only on masked (unknown) cells.\"\"\"\n",
        "    # preds: (B, 81, 9)\n",
        "    # targets: (B, 81)\n",
        "    # mask: (B, 81)\n",
        "    loss = F.cross_entropy(preds.reshape(-1, 9), targets.reshape(-1), reduction='none')\n",
        "    loss = loss.reshape(targets.shape)\n",
        "    masked_loss = loss * mask.float()\n",
        "    return masked_loss.sum() / (mask.sum() + 1e-6)\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions, targets, mask):\n",
        "    \"\"\"Compute cell-level and puzzle-level accuracy.\n",
        "\n",
        "    Args:\n",
        "        predictions: Model output logits, shape (batch, 81, 9).\n",
        "        targets: Target class indices, shape (batch, 81).\n",
        "        mask: Boolean mask, shape (batch, 81). True = cell needs prediction.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cell_accuracy, puzzle_accuracy).\n",
        "    \"\"\"\n",
        "    # Get predicted classes (argmax over the 9 classes)\n",
        "    predicted_classes = predictions.argmax(dim=-1)  # (batch, 81)\n",
        "\n",
        "    # Cell accuracy: correct predictions among masked cells\n",
        "    correct = (predicted_classes == targets) & mask  # Both correct AND masked\n",
        "    cell_accuracy = correct.sum().float() / (mask.sum().float() + 1e-6)\n",
        "\n",
        "    # Puzzle accuracy: all masked cells correct for each puzzle\n",
        "    correct_per_puzzle = correct.sum(dim=1)  # (batch,) - correct cells per puzzle\n",
        "    masked_per_puzzle = mask.sum(dim=1)       # (batch,) - masked cells per puzzle\n",
        "    puzzles_solved = (correct_per_puzzle == masked_per_puzzle).float()\n",
        "    puzzle_accuracy = puzzles_solved.mean()\n",
        "\n",
        "    return cell_accuracy.item(), puzzle_accuracy.item()\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, optimizer, scaler):\n",
        "    \"\"\"Train for one epoch and return metrics.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_cell_acc = 0\n",
        "    total_puzzle_acc = 0\n",
        "    num_batches = 0\n",
        "    pbar = tqdm(loader)\n",
        "    \n",
        "    for batch in pbar:\n",
        "        q = batch['question'].to(device)\n",
        "        a = batch['answer'].to(device)\n",
        "        m = batch['mask'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Mixed Precision Context\n",
        "        if device.type == 'cuda':\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                preds = model(q)\n",
        "                loss = masked_loss(preds, a, m)\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        \n",
        "        elif device.type == 'mps': # Mac Optimized\n",
        "             with torch.autocast(device_type='mps', dtype=torch.float16):\n",
        "                 preds = model(q)\n",
        "                 loss = masked_loss(preds, a, m)\n",
        "             loss.backward()\n",
        "             optimizer.step()\n",
        "             \n",
        "        else: # CPU\n",
        "            preds = model(q)\n",
        "            loss = masked_loss(preds, a, m)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        # Compute accuracy\n",
        "        with torch.no_grad():\n",
        "            cell_acc, puzzle_acc = compute_accuracy(preds, a, m)\n",
        "            \n",
        "        total_loss += loss.item()\n",
        "        total_cell_acc += cell_acc\n",
        "        total_puzzle_acc += puzzle_acc\n",
        "        num_batches += 1\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'cell_acc': f'{cell_acc:.2%}',\n",
        "            'puzzle_acc': f'{puzzle_acc:.2%}',\n",
        "        })\n",
        "        \n",
        "    return {\n",
        "        'loss': total_loss / num_batches,\n",
        "        'cell_accuracy': total_cell_acc / num_batches,\n",
        "        'puzzle_accuracy': total_puzzle_acc / num_batches,\n",
        "    }\n",
        "\n",
        "\n",
        "# Start Training\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    metrics = train_epoch(model, train_loader, optimizer, scaler)\n",
        "    print(f\"Average Loss: {metrics['loss']:.4f}, \"\n",
        "          f\"Cell Acc: {metrics['cell_accuracy']:.2%}, \"\n",
        "          f\"Puzzle Acc: {metrics['puzzle_accuracy']:.2%}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
