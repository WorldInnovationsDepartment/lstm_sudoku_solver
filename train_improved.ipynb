{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Sudoku Solver Training\n",
    "\n",
    "This notebook implements an optimized pipeline for training a Sudoku solver. \n",
    "Key improvements over the original:\n",
    "1.  **Offline Binary Data**: Pre-processes data to simple integer arrays (`.npy`), avoiding slow string parsing during training.\n",
    "2.  **Embeddings**: Uses `nn.Embedding` instead of One-Hot encoding for better memory efficiency.\n",
    "3.  **Data Augmentation**: Implements Sudoku-valid permutations (rows, cols, digits) to expand the dataset.\n",
    "4.  **Mixed Precision**: Uses `torch.amp` for faster training.\n",
    "5.  **Large Batch Size**: Enabled by the above optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing\n",
    "We defines functions to download, augment, and save the dataset as binary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def shuffle_sudoku(board_flat, solution_flat):\n",
    "    \"\"\"Apply valid Sudoku transformations (permutations) to a board and solution.\"\"\"\n",
    "    # Reshape to 9x9\n",
    "    board = board_flat.reshape(9, 9)\n",
    "    sol = solution_flat.reshape(9, 9)\n",
    "    \n",
    "    # 1. Permute digits (1-9)\n",
    "    # Create a mapping: 0->0 (unknown), 1-9 -> permuted 1-9\n",
    "    digit_map = np.arange(10)\n",
    "    digit_map[1:] = np.random.permutation(np.arange(1, 10))\n",
    "    \n",
    "    # 2. Random Transpose\n",
    "    if np.random.rand() < 0.5:\n",
    "        board = board.T\n",
    "        sol = sol.T\n",
    "        \n",
    "    # 3. Permute Bands (groups of 3 rows)\n",
    "    bands = np.random.permutation(3)\n",
    "    row_perm = np.concatenate([b * 3 + np.random.permutation(3) for b in bands])\n",
    "    \n",
    "    # 4. Permute Stacks (groups of 3 cols)\n",
    "    stacks = np.random.permutation(3)\n",
    "    col_perm = np.concatenate([s * 3 + np.random.permutation(3) for s in stacks])\n",
    "    \n",
    "    # Apply permutations\n",
    "    # We can do this by indexing: new_board[i, j] = old_board[row_perm[i], col_perm[j]]\n",
    "    # Or simpler: reorder rows, then reorder cols\n",
    "    board = board[row_perm, :][:, col_perm]\n",
    "    sol = sol[row_perm, :][:, col_perm]\n",
    "    \n",
    "    # Map digits\n",
    "    board = digit_map[board]\n",
    "    sol = digit_map[sol]\n",
    "    \n",
    "    return board.flatten(), sol.flatten()\n",
    "\n",
    "def preprocess_dataset(output_dir=\"data/processed\", num_aug=1):\n",
    "    \"\"\"Download, filter, augment, and save dataset as .npy.\"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"Dataset already exists at {output_dir}. Skipping generation.\")\n",
    "        return\n",
    "        \n",
    "    print(\"Loading dataset from HuggingFace...\")\n",
    "    ds = load_dataset(\"sapientinc/sudoku-extreme\")\n",
    "    \n",
    "    # Filter easy sources\n",
    "    easy_sources = ['puzzles0_kaggle', 'puzzles1_unbiased', 'puzzles2_17_clue']\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        print(f\"Processing {split} split...\")\n",
    "        # Filter\n",
    "        split_ds = ds[split].filter(lambda x: x['source'] in easy_sources)\n",
    "        \n",
    "        questions = []\n",
    "        answers = []\n",
    "        \n",
    "        print(\"Converting to integers and augmenting...\")\n",
    "        for item in tqdm(split_ds):\n",
    "            # Parse strings one last time\n",
    "            # '.' -> 0, '1'-'9' -> 1-9\n",
    "            q = np.array([0 if c == '.' else int(c) for c in item['question']], dtype=np.uint8)\n",
    "            # Answer is 1-9. We keep it 1-9 for now (0 means N/A if needed, but answers are full)\n",
    "            # Original code shifted answer to 0-8. Here we keep 1-9 to match input features,\n",
    "            # but will shift for loss calculation if needed.\n",
    "            a = np.array([int(c) for c in item['answer']], dtype=np.uint8)\n",
    "            \n",
    "            # Original data\n",
    "            questions.append(q)\n",
    "            answers.append(a)\n",
    "            \n",
    "            # Augmentations (only for train)\n",
    "            if split == 'train' and num_aug > 0:\n",
    "                for _ in range(num_aug):\n",
    "                    q_aug, a_aug = shuffle_sudoku(q, a)\n",
    "                    questions.append(q_aug)\n",
    "                    answers.append(a_aug)\n",
    "        \n",
    "        # Save as .npy\n",
    "        q_arr = np.array(questions, dtype=np.uint8)\n",
    "        a_arr = np.array(answers, dtype=np.uint8)\n",
    "        \n",
    "        print(f\"Saving {len(q_arr)} samples to {output_dir}...\")\n",
    "        np.save(os.path.join(output_dir, f\"{split}_questions.npy\"), q_arr)\n",
    "        np.save(os.path.join(output_dir, f\"{split}_answers.npy\"), a_arr)\n",
    "\n",
    "# Run preprocessing (will skip if already exists)\n",
    "preprocess_dataset(num_aug=1)  # 1 augmentation -> 2x dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimized Dataset Class\n",
    "Loads data directly from memory-mapped `.npy` files. Instant access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastSudokuDataset(Dataset):\n",
    "    def __init__(self, data_dir, split):\n",
    "        self.questions = np.load(os.path.join(data_dir, f\"{split}_questions.npy\"))\n",
    "        self.answers = np.load(os.path.join(data_dir, f\"{split}_answers.npy\"))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Data is already uint8 0-9\n",
    "        # q: (81,)\n",
    "        # a: (81,)\n",
    "        q = self.questions[idx].astype(np.int64) # Long for embedding\n",
    "        a = self.answers[idx].astype(np.int64)\n",
    "        \n",
    "        # Create mask (where q was 0)\n",
    "        # Note: In embedding, we can just feed 0. \n",
    "        # But for loss we need mask.\n",
    "        mask = (q == 0)\n",
    "        \n",
    "        # Target for Loss: PyTorch CrossEntropy expects 0-8 for classes 0-8.\n",
    "        # Our answers are 1-9. So we subtract 1.\n",
    "        target = a - 1\n",
    "        \n",
    "        return {\n",
    "            'question': torch.from_numpy(q),    # (81,) Indices 0-9\n",
    "            'answer': torch.from_numpy(target), # (81,) Indices 0-8\n",
    "            'mask': torch.from_numpy(mask)      # (81,) Bool\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improved Model with Embeddings\n",
    "Replaced One-Hot input with `nn.Embedding`. \n",
    "- Input dimension dropped from `(B, 81, 10)` to `(B, 81)` indices.\n",
    "- Memory usage significantly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SudokuLSTM_Improved(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=512,\n",
    "        num_layers=6,\n",
    "        dropout=0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Embedding layer\n",
    "        # 10 possible values in input: 0 (unknown) + 1-9 (digits)\n",
    "        self.embedding = nn.Embedding(10, hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # Output maps to 9 classes (digits 1-9)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 81) indices\n",
    "        # embed: (batch, 81, hidden)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # lstm_out: (batch, 81, hidden * 2)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # out: (batch, 81, 9)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimized Training Loop\n",
    "- **AMP (Automatic Mixed Precision)**: `torch.amp.autocast`\n",
    "- **Larger Batch Size**\n",
    "- **Gradient Clipping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "BATCH_SIZE = 1024  # Increased from 128\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 20    # Can run many more due to speed\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Data\n",
    "train_ds = FastSudokuDataset(\"data/processed\", \"train\")\n",
    "test_ds = FastSudokuDataset(\"data/processed\", \"test\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Model Setup\n",
    "model = SudokuLSTM_Improved().to(device)\n",
    "\n",
    "# Compile model if available (Linux/CUDA usually)\n",
    "if hasattr(torch, 'compile') and device.type == 'cuda':\n",
    "    print(\"Compiling model...\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "\n",
    "def masked_loss(preds, targets, mask):\n",
    "    # preds: (B, 81, 9)\n",
    "    # targets: (B, 81)\n",
    "    # mask: (B, 81)\n",
    "    loss = F.cross_entropy(preds.reshape(-1, 9), targets.reshape(-1), reduction='none')\n",
    "    loss = loss.reshape(targets.shape)\n",
    "    masked_loss = loss * mask.float()\n",
    "    return masked_loss.sum() / (mask.sum() + 1e-6)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(loader)\n",
    "    \n",
    "    for batch in pbar:\n",
    "        q = batch['question'].to(device)\n",
    "        a = batch['answer'].to(device)\n",
    "        m = batch['mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed Precision Context\n",
    "        if device.type == 'cuda':\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                preds = model(q)\n",
    "                loss = masked_loss(preds, a, m)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        elif device.type == 'mps': # Mac Optimized\n",
    "             with torch.autocast(device_type='mps', dtype=torch.float16):\n",
    "                 preds = model(q)\n",
    "                 loss = masked_loss(preds, a, m)\n",
    "             loss.backward()\n",
    "             optimizer.step()\n",
    "             \n",
    "        else: # CPU\n",
    "            preds = model(q)\n",
    "            loss = masked_loss(preds, a, m)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Start Training\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    loss = train_epoch(model, train_loader, optimizer, scaler)\n",
    "    print(f\"Average Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
